{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bXGQynTJq0l",
        "outputId": "0c9a5650-db60-42bc-d43c-fbed4313c0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been successfully written to combined_companies.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Web scraping for the first website\n",
        "url1 = 'https://builtin.com/companies/tech/aws-companies?page={}'\n",
        "base_url1 = 'https://builtin.com/companies/tech/aws-companies?page={}'\n",
        "company_data1 = []\n",
        "\n",
        "for page_number in range(1, 44):\n",
        "    current_url = base_url1.format(page_number)\n",
        "    r1 = requests.get(current_url)\n",
        "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
        "    company_list1 = soup1.find_all(\"h2\", class_=\"fw-extrabold fs-xl hover-underline d-inline-block company-title-clamp mb-0\")\n",
        "    company_data1.extend([company.get_text(strip=True) for company in company_list1])\n",
        "\n",
        "# Web scraping for the second website\n",
        "url2 = 'https://www.forbes.com/lists/cloud100/?sh=1a9a50077d9c'\n",
        "r2 = requests.get(url2)\n",
        "soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
        "company_list2 = soup2.find_all(\"div\", class_=\"organizationName second table-cell company\")\n",
        "company_data2 = [company.get_text(strip=True) for company in company_list2]\n",
        "\n",
        "# Web scraping for the third website\n",
        "url3 = 'https://www.builtinla.com/companies/tech/aws-companies'\n",
        "r3 = requests.get(url3)\n",
        "soup3 = BeautifulSoup(r3.text, 'html.parser')\n",
        "company_list3 = soup2.find_all(\"h2\", class_=\"fw-extrabold fs-xl hover-underline d-inline-block company-title-clamp mb-0\")\n",
        "company_data3 = [company.get_text(strip=True) for company in company_list3]\n",
        "\n",
        "# Combine the company data from both sources\n",
        "all_company_data = company_data1 + company_data2\n",
        "\n",
        "# Create a CSV file and write the data\n",
        "csv_file_path = 'combined_companies.csv'\n",
        "with open(csv_file_path, 'w', newline='') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow(['Company Name'])\n",
        "\n",
        "    # Write the combined company data to the CSV file\n",
        "    for company in all_company_data:\n",
        "        csv_writer.writerow([company])\n",
        "\n",
        "print(f\"Data has been successfully written to {csv_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPav7xhaNnys",
        "outputId": "44c5dbbe-abd9-4d6c-c462-a2cc09c48fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Company names have been successfully written to company_names.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape company names from a given URL\n",
        "def scrape_company_names(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    company_list = soup.find_all(\"h2\", class_=\"fw-extrabold fs-xl hover-underline d-inline-block company-title-clamp mb-0\")\n",
        "    return [company.get_text(strip=True) for company in company_list]\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.builtinla.com/companies?page={}'\n",
        "\n",
        "# Number of pages to scrape (382 in this case)\n",
        "total_pages = 382\n",
        "\n",
        "# List to store company names\n",
        "company_names = []\n",
        "\n",
        "# Iterate over each page and scrape company names\n",
        "for page_number in range(1, total_pages + 1):\n",
        "    current_url = base_url.format(page_number)\n",
        "    company_names.extend(scrape_company_names(current_url))\n",
        "\n",
        "# Save company names to CSV file\n",
        "csv_file_path = 'company_names.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow(['Company Name'])\n",
        "\n",
        "    # Write the company names to the CSV file\n",
        "    for company_name in company_names:\n",
        "        csv_writer.writerow([company_name])\n",
        "\n",
        "print(f\"Company names have been successfully written to {csv_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCIsi5nN7K5a",
        "outputId": "4c6ed7a7-fd0b-4519-89ae-558993b8765f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Companies and their links have been successfully written to companies_data.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape links and company names from a given URL\n",
        "def scrape_data(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Scrape links\n",
        "    link_list = soup.find_all(\"a\", class_=\"info-item company-website-link\")  # Adjust the class based on your HTML structure\n",
        "    links = [link['href'] for link in link_list]\n",
        "\n",
        "    # Scrape company names\n",
        "    company_list = soup.find_all(\"h2\", class_=\"fw-extrabold fs-xl hover-underline d-inline-block company-title-clamp mb-0\")\n",
        "    company_names = [company.get_text(strip=True) for company in company_list]\n",
        "\n",
        "    return list(zip(company_names, links))\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.builtinla.com/companies?page={}'\n",
        "\n",
        "# Number of pages to scrape (382 in this case)\n",
        "total_pages = 382\n",
        "\n",
        "# List to store companies and their links\n",
        "companies_data = []\n",
        "\n",
        "# Iterate over each page and scrape data\n",
        "for page_number in range(1, total_pages + 1):\n",
        "    current_url = base_url.format(page_number)\n",
        "    companies_data.extend(scrape_data(current_url))\n",
        "\n",
        "# Print companies and their links\n",
        "for company, link in companies_data:\n",
        "    print(f\"Company: {company}\\nLink: {link}\\n\")\n",
        "\n",
        "# Save companies and their links to CSV file\n",
        "csv_file_path = 'companies_data.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow(['Company Name', 'Link'])\n",
        "\n",
        "    # Write the companies and links to the CSV file\n",
        "    csv_writer.writerows(companies_data)\n",
        "\n",
        "print(f\"Companies and their links have been successfully written to {csv_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t8ROL2Z97tO",
        "outputId": "58a271e9-14c8-40ad-e8c6-86ca1a4f6bd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Company names and links have been successfully written to company_data.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape company names from a given URL\n",
        "def scrape_company_names(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Adjust the class based on your HTML structure\n",
        "    company_list = soup.find_all(\"h2\", class_=\"fw-extrabold fs-xl hover-underline d-inline-block company-title-clamp mb-0\")\n",
        "\n",
        "    return [company.get_text(strip=True) for company in company_list]\n",
        "\n",
        "def scrape_company_links(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Adjust the class based on your HTML structure\n",
        "    company_link = soup.find_all(\"a\", class_=\"btn btn-lg btn-outline-primary w-100\")\n",
        "\n",
        "    return [company.get_text(strip=True) for company in company_link]\n",
        "\n",
        "# Replace this URL with the actual URL of the page you want to scrape\n",
        "url = 'https://www.builtinla.com/companies?page=1'\n",
        "\n",
        "\n",
        "# Scrape company names and links\n",
        "company_names = scrape_company_names(url)\n",
        "company_links = scrape_company_links(url)\n",
        "\n",
        "# Save company data to CSV file\n",
        "csv_file_path = 'company_data.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow(['Company Name', 'Company Link'])\n",
        "\n",
        "    # Write the company data to the CSV file\n",
        "    for company_name, company_link in zip(company_names, company_links):\n",
        "        csv_writer.writerow([company_name, company_link])\n",
        "\n",
        "print(f\"Company names and links have been successfully written to {csv_file_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNIgSPdMIusn",
        "outputId": "a096a3cd-7a7b-4013-a1d0-45fea5945ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Company links have been successfully written to company_links.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape company names and links from a given URL\n",
        "def scrape_company_links(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Adjust the class based on your HTML structure\n",
        "    company_links = soup.find_all(\"a\", class_=\"btn btn-lg btn-outline-primary w-100\")\n",
        "\n",
        "    return [company.get(\"href\") for company in company_links]\n",
        "\n",
        "# Replace this URL with the actual URL of the page you want to scrape\n",
        "url = 'https://www.builtinla.com/companies?page=1'\n",
        "\n",
        "# Scrape company links\n",
        "company_links = scrape_company_links(url)\n",
        "\n",
        "# Save company links to CSV file\n",
        "csv_file_path = 'company_links.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow(['Company Link'])\n",
        "\n",
        "    # Write the company links to the CSV file\n",
        "    for link in company_links:\n",
        "        csv_writer.writerow([link])\n",
        "\n",
        "print(f\"Company links have been successfully written to {csv_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIA6x0zqKJ-s",
        "outputId": "26e253df-c2ad-4835-b168-8a206fadab4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All company data (names and links) have been successfully written to all_companies_data.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape company names and links from a given URL\n",
        "def scrape_company_data(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Scraping company names\n",
        "    company_names = []\n",
        "    company_list = soup.find_all(\"h2\", class_=\"fw-extrabold fs-xl hover-underline d-inline-block company-title-clamp mb-0\")\n",
        "    company_names = [company.get_text(strip=True) for company in company_list]\n",
        "\n",
        "    # Scraping company links\n",
        "    company_links = []\n",
        "    company_elements = soup.find_all(\"a\", class_=\"btn btn-lg btn-outline-primary w-100\")\n",
        "    company_links = [company.get(\"href\") for company in company_elements]\n",
        "\n",
        "    return list(zip(company_names, company_links))\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.builtinla.com/companies?page={}'\n",
        "\n",
        "# Number of pages to scrape (382 in this case)\n",
        "total_pages = 382\n",
        "\n",
        "# List to store company data (names and links)\n",
        "all_companies_data = []\n",
        "\n",
        "# Iterate over each page and scrape company data\n",
        "for page_number in range(1, total_pages + 1):\n",
        "    current_url = base_url.format(page_number)\n",
        "    companies_data = scrape_company_data(current_url)\n",
        "    all_companies_data.extend(companies_data)\n",
        "\n",
        "# Save all company data to CSV file\n",
        "csv_file_path = 'all_companies_data.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow(['Company Name', 'Company Link'])\n",
        "\n",
        "    # Write the company data to the CSV file\n",
        "    for company in all_companies_data:\n",
        "        csv_writer.writerow(company)\n",
        "\n",
        "print(f\"All company data (names and links) have been successfully written to {csv_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksQLpzs9SNhD",
        "outputId": "0fa847e0-6080-4752-c2a9-37ee78613b14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.16.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.23.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "chrome_driver_path = '/content/drive/MyDrive/chromedriver'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-pnu7tubyrE"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape official link from the company detail page\n",
        "def scrape_official_link(company_detail_url):\n",
        "    # Add your logic to extract the official link based on the structure of the company detail page\n",
        "    # This is a placeholder, modify it according to the actual structure\n",
        "    official_link = \"Not available\"\n",
        "\n",
        "    # Example:\n",
        "    # r = requests.get(company_detail_url)\n",
        "    # soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    # official_link = soup.find(\"a\", class_=\"official-link-class\").get(\"href\")\n",
        "\n",
        "    return official_link\n",
        "\n",
        "# Function to scrape company names and links from a given URL\n",
        "def scrape_company_data(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Scraping company names\n",
        "    company_names = []\n",
        "    company_list = soup.find_all(\"h2\", class_=\"fw-extrabold fs-xl hover-underline d-inline-block company-title-clamp mb-0\")\n",
        "    company_names = [company.get_text(strip=True) for company in company_list]\n",
        "\n",
        "    # Scraping company links\n",
        "    company_links = []\n",
        "    company_elements = soup.find_all(\"a\", class_=\"btn btn-lg btn-outline-primary w-100\")\n",
        "    company_links = [company.get(\"href\") for company in company_elements]\n",
        "\n",
        "    # Combine names and links\n",
        "    companies_data = list(zip(company_names, company_links))\n",
        "\n",
        "    # Scraping official links from each company's detail page\n",
        "    for i, (company_name, company_detail_url) in enumerate(companies_data):\n",
        "        print(f\"Scraping {i + 1}/{len(companies_data)}: {company_name}\")\n",
        "        official_link = scrape_official_link(company_detail_url)\n",
        "        companies_data[i] += (official_link,)\n",
        "\n",
        "    return companies_data\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.builtinla.com/companies?page={}'\n",
        "\n",
        "# Number of pages to scrape (382 in this case)\n",
        "total_pages = 382\n",
        "\n",
        "# List to store company data (names, links, and official links)\n",
        "all_companies_data = []\n",
        "\n",
        "# Iterate over each page and scrape company data\n",
        "for page_number in range(1, total_pages + 1):\n",
        "    current_url = base_url.format(page_number)\n",
        "    companies_data = scrape_company_data(current_url)\n",
        "    all_companies_data.extend(companies_data)\n",
        "\n",
        "# Save all company data to CSV file\n",
        "csv_file_path = 'all_companies_data.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow(['Company Name', 'Company Link', 'Official Link'])\n",
        "\n",
        "    # Write the company data to the CSV file\n",
        "    for company in all_companies_data:\n",
        "        csv_writer.writerow(company)\n",
        "\n",
        "print(f\"All company data (names, links, and official links) have been successfully written to {csv_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zyrg1r2D4EWK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQFSlgXngWqf",
        "outputId": "79eaf6a7-92e0-43e8-aa93-6e02618b4e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All company data (names, links, and official links) have been successfully written to all_companies_data.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape company names, links, and official links from a given URL\n",
        "def scrape_company_data(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Scraping company names and links\n",
        "    company_names = []\n",
        "    company_links = []\n",
        "    company_list = soup.find_all(\"h2\", class_=\"fw-extrabold fs-xl hover-underline d-inline-block company-title-clamp mb-0\")\n",
        "    for company in company_list:\n",
        "        company_name = company.get_text(strip=True)\n",
        "        company_link_element = company.find(\"a\")\n",
        "        company_link = company_link_element[\"href\"] if company_link_element else None\n",
        "        company_names.append(company_name)\n",
        "        company_links.append(company_link)\n",
        "\n",
        "    # Scraping official links from company detail pages\n",
        "    official_links = []\n",
        "    for company_link in company_links:\n",
        "        if company_link:\n",
        "            company_detail_page = requests.get(company_link)\n",
        "            company_detail_soup = BeautifulSoup(company_detail_page.text, 'html.parser')\n",
        "            official_link_element = company_detail_soup.find(\"a\", text=\"Official Website\")\n",
        "            official_link = official_link_element[\"href\"] if official_link_element else \"Not available\"\n",
        "            official_links.append(official_link)\n",
        "        else:\n",
        "            official_links.append(\"Not available\")\n",
        "\n",
        "    # Combine names, links, and official links\n",
        "    companies_data = list(zip(company_names, company_links, official_links))\n",
        "\n",
        "    return companies_data\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.builtinla.com/companies?page={}'\n",
        "\n",
        "# Number of pages to scrape (382 in this case)\n",
        "total_pages = 382\n",
        "\n",
        "# List to store company data (names, links, and official links)\n",
        "all_companies_data = []\n",
        "\n",
        "# Iterate over each page and scrape company data\n",
        "for page_number in range(1, total_pages + 1):\n",
        "    current_url = base_url.format(page_number)\n",
        "    companies_data = scrape_company_data(current_url)\n",
        "    all_companies_data.extend(companies_data)\n",
        "\n",
        "# Save all company data to CSV file\n",
        "csv_file_path = 'all_companies_data.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow(['Company Name', 'Company Link', 'Official Link'])\n",
        "\n",
        "    # Write the company data to the CSV file\n",
        "    for company in all_companies_data:\n",
        "        csv_writer.writerow(company)\n",
        "\n",
        "print(f\"All company data (names, links, and official links) have been successfully written to {csv_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxMBVZdZ4GR2",
        "outputId": "e800beff-f5cd-4dff-eb34-ba3bd1f12df6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Company Link: http://www.aceable.com\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# Find the anchor tag with the specified class\n",
        "company_link_element = soup.find('a', class_='info-item company-website-link')\n",
        "\n",
        "# Extract the href attribute (company link)\n",
        "company_link = company_link_element['href'] if company_link_element else None\n",
        "\n",
        "# Print the company link\n",
        "print(\"Company Link:\", company_link)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71Q2W5_Z_o_x"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape company links from a given URL\n",
        "def scrape_company_links(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Scraping company links\n",
        "    company_links = []\n",
        "    company_elements = soup.find_all(\"a\", class_=\"btn btn-lg btn-outline-primary w-100\")\n",
        "    company_links = [company.get(\"href\") for company in company_elements]\n",
        "\n",
        "    return company_links\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.builtinla.com/companies?page={}'\n",
        "\n",
        "# Number of pages to scrape (382 in this case)\n",
        "total_pages = 382\n",
        "\n",
        "# List to store company links\n",
        "all_company_links = []\n",
        "\n",
        "# Iterate over each page and scrape company links\n",
        "for page_number in range(1, total_pages + 1):\n",
        "    current_url = base_url.format(page_number)\n",
        "    company_links = scrape_company_links(current_url)\n",
        "    all_company_links.extend(company_links)\n",
        "\n",
        "# Print all company links\n",
        "for i, link in enumerate(all_company_links, start=1):\n",
        "    print(f\"Company Link {i}: {link}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyu-1uhI7AIk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape company links from a given URL\n",
        "def scrape_company_links(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Scraping company links\n",
        "    company_links = []\n",
        "    company_elements = soup.find_all(\"a\", class_=\"btn btn-lg btn-outline-primary w-100\")\n",
        "\n",
        "    for company in company_elements:\n",
        "        try:\n",
        "            company_link = company.get(\"href\")\n",
        "            company_links.append(company_link)\n",
        "        except TypeError:\n",
        "            print(f\"Failed to retrieve data from {url}\")\n",
        "            company_links.append(None)\n",
        "\n",
        "    return company_links\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.builtinla.com/companies?page={}'\n",
        "\n",
        "# Number of pages to scrape (382 in this case)\n",
        "total_pages = 382\n",
        "\n",
        "# List to store company links\n",
        "all_company_links = []\n",
        "\n",
        "# Iterate over each page and scrape company links\n",
        "for page_number in range(1, total_pages + 1):\n",
        "    current_url = base_url.format(page_number)\n",
        "    company_links = scrape_company_links(current_url)\n",
        "    all_company_links.extend(company_links)\n",
        "\n",
        "# Print all company links\n",
        "for i, link in enumerate(all_company_links, start=1):\n",
        "    print(f\"Company Link {i}: {link}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pm_C4HBDDwx",
        "outputId": "dad149a9-59ce-4ee4-b42d-cc7bd11cc0da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All company data (links and official links) have been successfully written to all_companies_data.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_company_links(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Scraping company links\n",
        "    company_links = []\n",
        "    company_elements = soup.find_all(\"a\", class_=\"btn btn-lg btn-outline-primary w-100\")\n",
        "    company_links = [company.get(\"href\") for company in company_elements]\n",
        "\n",
        "    return company_links\n",
        "\n",
        "def scrape_official_link(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Find the anchor tag with the specified class\n",
        "    company_link_element = soup.find('a', class_='info-item company-website-link')\n",
        "\n",
        "    # Extract the href attribute (company link)\n",
        "    company_link = company_link_element['href'] if company_link_element else None\n",
        "\n",
        "    return company_link\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.builtinla.com/companies?page={}'\n",
        "\n",
        "# Number of pages to scrape (382 in this case)\n",
        "total_pages = 3\n",
        "\n",
        "# List to store company links\n",
        "all_company_links = []\n",
        "\n",
        "# Iterate over each page and scrape company links\n",
        "for page_number in range(1, total_pages + 1):\n",
        "    current_url = base_url.format(page_number)\n",
        "    company_links = scrape_company_links(current_url)\n",
        "    all_company_links.extend(company_links)\n",
        "\n",
        "# List to store company data (names, links, and official links)\n",
        "all_companies_data = []\n",
        "\n",
        "# Iterate through the list of company links and scrape official company links\n",
        "for company_link in all_company_links:\n",
        "    official_link = scrape_official_link(company_link)\n",
        "    all_companies_data.append((company_link, official_link))\n",
        "\n",
        "# Save all company data to CSV file\n",
        "csv_file_path = 'all_companies_data.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow(['Company Link', 'Official Link'])\n",
        "\n",
        "    # Write the company data to the CSV file\n",
        "    for company_data in all_companies_data:\n",
        "        csv_writer.writerow(company_data)\n",
        "\n",
        "print(f\"All company data (links and official links) have been successfully written to {csv_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape company names and links from a given URL\n",
        "def scrape_company_data(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Scraping company names\n",
        "    company_names = []\n",
        "    company_list = soup.find_all(\"h2\", class_=\"fw-extrabold fs-xl hover-underline d-inline-block company-title-clamp mb-0\")\n",
        "    company_names = [company.get_text(strip=True) for company in company_list]\n",
        "\n",
        "    # Scraping company links\n",
        "    company_links = []\n",
        "    company_elements = soup.find_all(\"a\", class_=\"btn btn-lg btn-outline-primary w-100\")\n",
        "    company_links = [company.get(\"href\") for company in company_elements]\n",
        "\n",
        "    return list(zip(company_names, company_links))\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.builtinla.com/companies?page={}'\n",
        "\n",
        "# Number of pages to scrape (let's scrape the first 10 pages, each page contains around 100 companies)\n",
        "pages_to_scrape = 10\n",
        "\n",
        "# List to store company data (names and links)\n",
        "all_companies_data = []\n",
        "\n",
        "# Iterate over each page and scrape company data\n",
        "for page_number in range(1, pages_to_scrape + 1):\n",
        "    current_url = base_url.format(page_number)\n",
        "    companies_data = scrape_company_data(current_url)\n",
        "    all_companies_data.extend(companies_data)\n",
        "\n",
        "# Save all company data to CSV file\n",
        "csv_file_path = 'all_companies_data.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow(['Company Name', 'Company Link'])\n",
        "\n",
        "    # Write the company data to the CSV file\n",
        "    for company in all_companies_data:\n",
        "        csv_writer.writerow(company)\n",
        "\n",
        "print(f\"The first 1000 companies' names and links have been successfully written to {csv_file_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dru4_2jcrSk3",
        "outputId": "1884292c-f275-4f73-c147-78c32a8a7a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 1000 companies' names and links have been successfully written to all_companies_data.csv.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}